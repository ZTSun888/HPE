# Real-time Two-Hand Pose Estimation and Jitter Mitigation in Live Video via a Plug-and-Play Network

## Abstract
Accurate two-hand pose estimation from live video inputs is crucial for various human-computer interaction applications. However, existing methods often overlook practical challenges such as the similarity in local appearance between hands, complex backgrounds, and inter-frame jitters in video predictions. To address these issues, we present a two-hand pose estimation framework tailored for RGB images and live videos. Our approach introduces a simple yet effective modification to backbone networks, enhancing hand region feature extraction and compatibility with mainstream architectures. To mitigate the challenges of annotating video datasets and reduce jitters in video estimates, we propose a plug-and-play self-supervised training module. This module leverages predictions from the image estimation network and adheres to physiological and temporal consistency principles. Furthermore, we generate a synthetic dataset featuring actual hand poses against complex backgrounds, extending an open-source large-scale dataset to better simulate non-laboratory environments. Experimental results on public datasets demonstrate the effectiveness of each component in our network, achieving competitive performance compared to state-of-the-art methods.


## Dataset Download
* https://pan.baidu.com/s/15wiPfMuLOoAtslrTI5ilsg?pwd=myqx


